{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "import pickle\n",
    "import dill\n",
    "import time\n",
    "import json\n",
    "\n",
    "# Import the libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set constants**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "DATASET_PATH = f\"../.data/miguel\"\n",
    "SRC_LANG = \"en\"\n",
    "TRG_LANG = \"es\"\n",
    "SOS_WORD = '<sos>'\n",
    "EOS_WORD = '<eos>'\n",
    "MAX_SEQ_LENGTH = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "\n",
    "To speed things up, I prefer to use torchtext directly in order to read the CSV files, preprocess\n",
    "them and tokenize each pair.\n",
    "\n",
    "I'm gonna use the tokenizer from Spacy, which is a Natural Language Processing library that is blazingly fast, suitable\n",
    "for large datasets, with support for many language and hundreds of features.\n",
    "\n",
    "This step can take a while but since I plan to save our tokenized datasets, it  won't be a problem.\n",
    "\n",
    "Also note that I'm converting everything to lowercase, and adding the `<sos>` and `<eos>` tokens to our pairs.\n",
    "\n",
    "***Note:** Keep in mind that I share (by reference) the SRC/TRG fields between the train, dev and test partitions.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scarrion/anaconda3/lib/python3.7/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/home/scarrion/anaconda3/lib/python3.7/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "SRC = data.Field(tokenize='spacy', tokenizer_language=SRC_LANG, init_token=SOS_WORD, eos_token=EOS_WORD, lower=True)\n",
    "TRG = data.Field(tokenize='spacy', tokenizer_language=TRG_LANG, init_token=SOS_WORD, eos_token=EOS_WORD, lower=True)\n",
    "data_fields = [('src', SRC), ('trg', TRG)]  # Shared fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/salvacarrion/.local/lib/python3.8/site-packages/torchtext/data/example.py:68: UserWarning: Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.', UserWarning)\n",
      "/home/salvacarrion/.local/lib/python3.8/site-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "497.2978286743164\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "train_data, dev_data, test_data = data.TabularDataset.splits(path=f'{DATASET_PATH}/preprocessed/',\n",
    "                                                             train='train.csv', validation='dev.csv', test='test.csv',\n",
    "                                                             format='csv', fields=data_fields, skip_header=True)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving preprocessed dataset\n",
    "\n",
    "Since preprocessing and tokenizing a big dataset can take time, I recommend to save it always, if possible, in other\n",
    "to save us some time when debugging our model.\n",
    "\n",
    "For torchtext, we'll need to save the preprocessed examples for the train, dev and test partitions, along with their\n",
    "fields. Depending on the case we can ignore the last part. Here, I'll save only the train fields with their\n",
    "vocabularies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1960641/1960641 [02:03<00:00, 15906.52it/s]\n",
      "100%|██████████| 3003/3003 [00:00<00:00, 17365.34it/s]\n",
      "100%|██████████| 3003/3003 [00:00<00:00, 16957.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123.44851589202881\n",
      "0.17477941513061523\n",
      "0.1789076328277588\n",
      "Tokenized datasets saved!\n"
     ]
    }
   ],
   "source": [
    "def save_dataset(dataset, savepath):\n",
    "    start = time.time()\n",
    "\n",
    "    total = len(dataset.examples)\n",
    "    with open(savepath, 'wb') as f:\n",
    "        # Save num. elements\n",
    "        umsgpack.pack(total, f)\n",
    "\n",
    "        # Save elements\n",
    "        for pair in tqdm(dataset.examples, total=total):\n",
    "            data = [pair.src, pair.trg]\n",
    "            umsgpack.pack(data, f)\n",
    "\n",
    "    end = time.time()\n",
    "    print(end - start)\n",
    "\n",
    "save_dataset(train_data, f\"{DATASET_PATH}/tokenized/train.msgpack\")\n",
    "save_dataset(dev_data, f\"{DATASET_PATH}/tokenized/dev.msgpack\")\n",
    "save_dataset(test_data, f\"{DATASET_PATH}/tokenized/test.msgpack\")\n",
    "print(\"Tokenized datasets saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1960641/1960641 [00:15<00:00, 125955.90it/s]\n",
      "100%|██████████| 3003/3003 [00:00<00:00, 106927.25it/s]\n",
      "100%|██████████| 3003/3003 [00:00<00:00, 134750.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.33510136604309\n",
      "0.034819602966308594\n",
      "0.023775815963745117\n",
      "Tokenized datasets saved!\n"
     ]
    }
   ],
   "source": [
    "def save_dataset2(dataset, savepath):\n",
    "    start = time.time()\n",
    "\n",
    "    total = len(dataset.examples)\n",
    "    with open(savepath, 'w') as f:\n",
    "        # Save num. elements\n",
    "        f.write(json.dumps(total))\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "        # Save elements\n",
    "        for pair in tqdm(dataset.examples, total=total):\n",
    "            data = [pair.src, pair.trg]\n",
    "            f.write(json.dumps(data))\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "    end = time.time()\n",
    "    print(end - start)\n",
    "\n",
    "save_dataset2(train_data, f\"{DATASET_PATH}/tokenized/train.json\")\n",
    "save_dataset2(dev_data, f\"{DATASET_PATH}/tokenized/dev.json\")\n",
    "save_dataset2(test_data, f\"{DATASET_PATH}/tokenized/test.json\")\n",
    "print(\"Tokenized datasets saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1960641/1960641 [00:05<00:00, 336883.06it/s] \n",
      "100%|██████████| 3003/3003 [00:00<00:00, 1696820.01it/s]\n",
      "100%|██████████| 3003/3003 [00:00<00:00, 1602684.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46.543951749801636\n",
      "0.05698347091674805\n",
      "0.05739021301269531\n",
      "Tokenized datasets saved!\n"
     ]
    }
   ],
   "source": [
    "def save_dataset3(dataset, savepath):\n",
    "    start = time.time()\n",
    "\n",
    "    total = len(dataset.examples)\n",
    "    # Collect pairs\n",
    "    examples = []\n",
    "    for pair in tqdm(dataset.examples, total=total):\n",
    "        data = [pair.src, pair.trg]\n",
    "        examples.append(data)\n",
    "\n",
    "    # Save\n",
    "    with open(savepath, 'w') as f:\n",
    "        json.dump(examples, f)\n",
    "\n",
    "    end = time.time()\n",
    "    print(end - start)\n",
    "\n",
    "save_dataset3(train_data, f\"{DATASET_PATH}/tokenized/train3.json\")\n",
    "save_dataset3(dev_data, f\"{DATASET_PATH}/tokenized/dev3.json\")\n",
    "save_dataset3(test_data, f\"{DATASET_PATH}/tokenized/test3.json\")\n",
    "print(\"Tokenized datasets saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def save_dataset4(dataset, savepath):\n",
    "    start = time.time()\n",
    "\n",
    "    # Save\n",
    "    torch.save(dataset.examples, savepath, pickle_module=dill)\n",
    "\n",
    "    end = time.time()\n",
    "    print(end - start)\n",
    "\n",
    "save_dataset4(train_data, f\"{DATASET_PATH}/tokenized/train4.pkl\")\n",
    "save_dataset4(dev_data, f\"{DATASET_PATH}/tokenized/dev4.pkl\")\n",
    "save_dataset4(test_data, f\"{DATASET_PATH}/tokenized/test4.pkl\")\n",
    "print(\"Tokenized datasets saved!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading preprocessed dataset\n",
    "\n",
    "Similary, we can also load the preprocessed datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1960641/1960641 [02:23<00:00, 13635.20it/s]\n",
      "100%|██████████| 3003/3003 [00:00<00:00, 15644.65it/s]\n",
      "100%|██████████| 3003/3003 [00:00<00:00, 15745.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143.79657816886902\n",
      "0.19379568099975586\n",
      "0.19238901138305664\n",
      "Tokenized datasets loaded!\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(filename):\n",
    "    start = time.time()\n",
    "\n",
    "    examples = []\n",
    "    with open(filename, 'rb') as f:\n",
    "        # Read num. elements\n",
    "        total = umsgpack.unpack(f)\n",
    "\n",
    "        # Save elements\n",
    "        for i in tqdm(range(total), total=total):\n",
    "            example = umsgpack.unpack(f)\n",
    "            examples.append(example)\n",
    "\n",
    "    end = time.time()\n",
    "    print(end - start)\n",
    "    return examples\n",
    "\n",
    "\n",
    "train_data = load_dataset(f\"{DATASET_PATH}/tokenized/train.msgpack\")\n",
    "dev_data = load_dataset(f\"{DATASET_PATH}/tokenized/dev.msgpack\")\n",
    "test_data = load_dataset(f\"{DATASET_PATH}/tokenized/test.msgpack\")\n",
    "print(\"Tokenized datasets loaded!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1951270/1951270 [00:28<00:00, 69237.70it/s] \n",
      "100%|██████████| 3001/3001 [00:00<00:00, 93488.51it/s]\n",
      "100%|██████████| 3001/3001 [00:00<00:00, 112420.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.1856951713562\n",
      "0.034444570541381836\n",
      "0.028983354568481445\n",
      "Tokenized datasets loaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def load_dataset2(filename):\n",
    "    start = time.time()\n",
    "\n",
    "    examples = []\n",
    "    with open(filename, 'rb') as f:\n",
    "        # Read num. elements\n",
    "        line = f.readline()\n",
    "        total = json.loads(line)\n",
    "\n",
    "        # Save elements\n",
    "        for i in tqdm(range(total), total=total):\n",
    "            line = f.readline()\n",
    "            example = json.loads(line)\n",
    "            examples.append(example)\n",
    "\n",
    "    end = time.time()\n",
    "    print(end - start)\n",
    "    return examples\n",
    "\n",
    "\n",
    "train_data = load_dataset2(f\"{DATASET_PATH}/tokenized/train.json\")\n",
    "dev_data = load_dataset2(f\"{DATASET_PATH}/tokenized/dev.json\")\n",
    "test_data = load_dataset2(f\"{DATASET_PATH}/tokenized/test.json\")\n",
    "print(\"Tokenized datasets loaded!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.435794591903687\n",
      "0.02357649803161621\n",
      "0.02375054359436035\n",
      "Tokenized datasets loaded!\n"
     ]
    }
   ],
   "source": [
    "def load_dataset3(filename):\n",
    "    start = time.time()\n",
    "\n",
    "    with open(filename, 'r') as f:\n",
    "        # Read num. elements\n",
    "        examples = json.load(f)\n",
    "\n",
    "    end = time.time()\n",
    "    print(end - start)\n",
    "    return examples\n",
    "\n",
    "\n",
    "train_data = load_dataset3(f\"{DATASET_PATH}/tokenized/train3.json\")\n",
    "dev_data = load_dataset3(f\"{DATASET_PATH}/tokenized/dev3.json\")\n",
    "test_data = load_dataset3(f\"{DATASET_PATH}/tokenized/test3.json\")\n",
    "print(\"Tokenized datasets loaded!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def load_dataset4(filename):\n",
    "    start = time.time()\n",
    "\n",
    "    # Load\n",
    "    examples = torch.load(filename, pickle_module=dill)\n",
    "\n",
    "    end = time.time()\n",
    "    print(end - start)\n",
    "    return examples\n",
    "\n",
    "train_data = load_dataset4(f\"{DATASET_PATH}/tokenized/train4.pkl\")\n",
    "dev_data = load_dataset4(f\"{DATASET_PATH}/tokenized/dev4.pkl\")\n",
    "test_data = load_dataset4(f\"{DATASET_PATH}/tokenized/test4.pkl\")\n",
    "print(\"Tokenized datasets saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Convert list of list to a list of Example()\n",
    "train_examples = [data.Example().fromlist(d, data_fields) for d in train_data]\n",
    "dev_examples = [data.Example().fromlist(d, data_fields) for d in dev_data]\n",
    "test_examples = [data.Example().fromlist(d, data_fields) for d in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets built!\n",
      "Total pairs:\n",
      "\t- Train: 1951270\n",
      "\t- Dev: 3001\n",
      "\t- Test: 3001\n"
     ]
    }
   ],
   "source": [
    "# Build dataset (\"examples\" passed by reference)\n",
    "train_data = data.Dataset(train_examples, data_fields)\n",
    "dev_data = data.Dataset(dev_examples, data_fields)\n",
    "test_data = data.Dataset(test_examples, data_fields)\n",
    "print(\"Datasets built!\")\n",
    "\n",
    "print(\"Total pairs:\")\n",
    "print(f\"\\t- Train: {len(train_data.examples)}\")\n",
    "print(f\"\\t- Dev: {len(dev_data.examples)}\")\n",
    "print(f\"\\t- Test: {len(test_data.examples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
