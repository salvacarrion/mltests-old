{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "import pickle\n",
    "import dill\n",
    "import umsgpack\n",
    "import time\n",
    "import json\n",
    "\n",
    "# Import the libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Set constants**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "DATASET_PATH = f\"../.data/miguel\"\n",
    "SRC_LANG = \"en\"\n",
    "TRG_LANG = \"es\"\n",
    "SOS_WORD = '<sos>'\n",
    "EOS_WORD = '<eos>'\n",
    "MAX_SEQ_LENGTH = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "\n",
    "To speed things up, I prefer to use torchtext directly in order to read the CSV files, preprocess\n",
    "them and tokenize each pair.\n",
    "\n",
    "I'm gonna use the tokenizer from Spacy, which is a Natural Language Processing library that is blazingly fast, suitable\n",
    "for large datasets, with support for many language and hundreds of features.\n",
    "\n",
    "This step can take a while but since I plan to save our tokenized datasets, it  won't be a problem.\n",
    "\n",
    "Also note that I'm converting everything to lowercase, and adding the `<sos>` and `<eos>` tokens to our pairs.\n",
    "\n",
    "***Note:** Keep in mind that I share (by reference) the SRC/TRG fields between the train, dev and test partitions.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/salvacarrion/.local/lib/python3.8/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/home/salvacarrion/.local/lib/python3.8/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "SRC = data.Field(tokenize='spacy', tokenizer_language=SRC_LANG, init_token=SOS_WORD, eos_token=EOS_WORD, lower=True)\n",
    "TRG = data.Field(tokenize='spacy', tokenizer_language=TRG_LANG, init_token=SOS_WORD, eos_token=EOS_WORD, lower=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/salvacarrion/.local/lib/python3.8/site-packages/torchtext/data/example.py:68: UserWarning: Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.', UserWarning)\n",
      "/home/salvacarrion/.local/lib/python3.8/site-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.', UserWarning)\n"
     ]
    }
   ],
   "source": [
    "data_fields = [('src', SRC), ('trg', TRG)]  # Shared fields\n",
    "train_data, dev_data, test_data = data.TabularDataset.splits(path=f'{DATASET_PATH}/preprocessed/',\n",
    "                                                             train='train.csv', validation='dev.csv', test='test.csv',\n",
    "                                                             format='csv', fields=data_fields, skip_header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Dataset exploration\n",
    "\n",
    "On the first part of the exploration we perform some sanity checks to make sure that our preprocessed dataset are valid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the number of pairs for each partition: train, dev and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Total pairs:\")\n",
    "print(f\"\\t- Train: {len(train_data.examples)}\")\n",
    "print(f\"\\t- Dev: {len(dev_data.examples)}\")\n",
    "print(f\"\\t- Test: {len(test_data.examples)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing tokens\n",
    "\n",
    "Now, we can the print the tokenized pairs to see what are we really doing. I usually like to print the first and last\n",
    "`n` pairs (and a few random ones).\n",
    "\n",
    "\n",
    "----\n",
    "**Old:**\n",
    "*Below we see a few things. First, the tokenization looks good. Nevertheless, we see that there are still a few\n",
    "things that can be annoying such as `\\xa0` or `\"`. Maybe I should normalize the strings to avoid those problems.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_pairs(pairs, indices):\n",
    "    for i, idx in enumerate(indices):\n",
    "        (src, trg) = pairs[idx].src, pairs[idx].trg\n",
    "        print(f\"#{i+1}: \" + \"-\"*20)\n",
    "        print(f\"src => {src}\")\n",
    "        print(f\"trg => {trg}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n=5\n",
    "print(\"Head: \" + \"#\"*20)\n",
    "print(\"(Firsts) Train dataset: \" + \"*\"*20)\n",
    "view_pairs(train_data.examples, indices=range(0, n))\n",
    "\n",
    "print(\"(Firsts) Dev dataset: \" + \"*\"*20)\n",
    "view_pairs(dev_data.examples, indices=range(0, n))\n",
    "\n",
    "print(\"(Firsts) Test dataset: \" + \"*\"*20)\n",
    "view_pairs(test_data.examples, indices=range(0, n))\n",
    "\n",
    "print(\"Tail: \" + \"#\"*20)\n",
    "print(\"(Lasts) Train dataset: \" + \"*\"*20)\n",
    "view_pairs(train_data.examples, indices=range(-1,-n-1,-1))\n",
    "\n",
    "print(\"(Lasts) Dev dataset: \" + \"*\"*20)\n",
    "view_pairs(dev_data.examples, indices=range(-1,-n-1,-1))\n",
    "\n",
    "print(\"(Lasts) Test dataset: \" + \"*\"*20)\n",
    "view_pairs(test_data.examples, indices=range(-1,-n-1,-1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Sequence lengths\n",
    "\n",
    "Since some of models that use of attention have a quadratic growth with regard the length of a sequence, we need to \n",
    "carefully study the lengths of the pairs in our dataset so that we can choose the maximum length of a sequence in a \n",
    "educated way.\n",
    "\n",
    "From the results below we observe a huge gap between pairs. This gap gets accentuated for the train and dev/test partitions. In the former, the gap is greater than in the later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Get lengths\n",
    "train_lenghts = np.array([(len(pair.src), len(pair.trg)) for pair in train_data.examples])\n",
    "dev_lenghts = np.array([(len(pair.src), len(pair.trg)) for pair in dev_data.examples])\n",
    "test_lenghts = np.array([(len(pair.src), len(pair.trg)) for pair in test_data.examples])\n",
    "\n",
    "# Get min lengths\n",
    "train_min_len = train_lenghts.min(axis=0).astype(int)\n",
    "dev_min_len = dev_lenghts.min(axis=0).astype(int)\n",
    "test_min_len = test_lenghts.min(axis=0).astype(int)\n",
    "\n",
    "\n",
    "# Get max lengths\n",
    "train_max_len = train_lenghts.max(axis=0).astype(int)\n",
    "dev_max_len = dev_lenghts.max(axis=0).astype(int)\n",
    "test_max_len = test_lenghts.max(axis=0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train => Range length: src={train_min_len[0]}-{train_max_len[0]} | trg={train_min_len[1]}-{train_max_len[1]}\")\n",
    "print(f\"Dev => Range length: src={dev_min_len[0]}-{dev_max_len[0]} | trg={dev_min_len[1]}-{dev_max_len[1]}\")\n",
    "print(f\"Test => Range length: src={test_min_len[0]}-{test_max_len[0]} | trg={test_min_len[1]}-{test_max_len[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking for zero-length sequences\n",
    "\n",
    "Additionally, I like to check if there is any sequence with length zero. This is more important that it looks, since an empty sequence could indicate an error or problem in the dataset which could lead to a waste of computational resources, a `RuntimeError` in the middle of the training or even jeopardising the training of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_no_sent = np.sum(train_lenghts == 0, axis=0).astype(int)\n",
    "dev_no_sent = np.sum(dev_lenghts == 0, axis=0).astype(int)\n",
    "ts_no_sent = np.sum(test_lenghts == 0, axis=0).astype(int)\n",
    "\n",
    "print(f\"Train => Sentences with no words: SRC={tr_no_sent[0]} | TRG={tr_no_sent[1]}\")\n",
    "print(f\"Dev => Sentences with no words: SRC={dev_no_sent[0]} | TRG={dev_no_sent[1]}\")\n",
    "print(f\"Test => Sentences with no words: SRC={ts_no_sent[0]} | TRG={ts_no_sent[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing extreme sequences\n",
    "\n",
    "To know the range in which the sequences are ranging is important, but it is even more important to understand what are the extreme pairs in that range. By studying these extreme cases, we can educatively tune our model to get the most of our dataset without compromissing their requirements too much.\n",
    "\n",
    "To get these extreme pairs, we simply keep the indices of the sorted lengths using `argsort`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def view_lengths(raw, indices):\n",
    "    for i, idx in enumerate(indices):\n",
    "        p1_src, p1_trg = raw[idx[0]].src, raw[idx[0]].trg  # Shortest src (+trg)\n",
    "        p2_src, p2_trg = raw[idx[1]].src, raw[idx[1]].trg  # Shortest trg (+src)\n",
    "\n",
    "        \n",
    "        print(f\"#{i+1}: \" + \"-\"*20)\n",
    "        print(f\"[Shortest SRC: idx={idx[0]}]\")\n",
    "        print(f\"\\t- src (len={len(p1_src)}): {p1_src} => {p1_trg}\")\n",
    "        \n",
    "        print(f\"[Shortest TRG: idx={idx[1]}]\")\n",
    "        print(f\"\\t- trg (len={len(p2_src)}): {p2_src} => {p2_trg}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Sort columns independently (ascending)\n",
    "train_sort_len_idx = np.argsort(train_lenghts, axis=0)\n",
    "dev_sort_len_idx = np.argsort(dev_lenghts, axis=0)\n",
    "test_sort_len_idx = np.argsort(test_lenghts, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we see which sentences contain the minimum and the maximum number of tokens. The shortests sequences correspond to word-to-word translations such as: `italy => italia` or `Informality => Informalidad`.\n",
    " \n",
    "**Notice** that columns in the array are sorted independely. This is because a source sequence can be a single word but its translation can be many (and viceversa), such as: `shopping => ir de compras`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n=5\n",
    "print(\"Shortest: \" + \"#\"*20)\n",
    "print(\"(Shortest) Train dataset: \" + \"*\"*20)\n",
    "view_lengths(train_data.examples, indices=train_sort_len_idx[:n])\n",
    "\n",
    "print(\"(Shortest) Dev dataset: \" + \"*\"*20)\n",
    "view_lengths(dev_data.examples, indices=dev_sort_len_idx[:n])\n",
    "\n",
    "print(\"(Shortest) Test dataset: \" + \"*\"*20)\n",
    "view_lengths(test_data.examples, indices=test_sort_len_idx[:n])\n",
    "\n",
    "print(\"Longest: \" + \"#\"*20)\n",
    "print(\"(Longest) Train dataset: \" + \"*\"*20)\n",
    "view_lengths(train_data.examples, indices=train_sort_len_idx[::-1][:n])\n",
    "\n",
    "print(\"(Longest) Dev dataset: \" + \"*\"*20)\n",
    "view_lengths(dev_data.examples, indices=dev_sort_len_idx[::-1][:n])\n",
    "\n",
    "print(\"(Longest) Test dataset: \" + \"*\"*20)\n",
    "view_lengths(test_data.examples, indices=test_sort_len_idx[::-1][:n])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting length distribution\n",
    "\n",
    "After printing a few examples of pairs, it is interesting to plot the length distribution of the different partitions so we can get an intuition on where to set the length cut.\n",
    "\n",
    "From the plots below, we see that the train partition come from a different source than our dev and test partitions. Luckily, this won't be a problem for setting out cut since the training partition is way bigger than our dev/test partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Get lengths\n",
    "tr_src_len = train_lenghts[:, 0]\n",
    "tr_trg_len = train_lenghts[:, 1]\n",
    "dev_src_len = dev_lenghts[:, 0]\n",
    "dev_trg_len = dev_lenghts[:, 1]\n",
    "ts_src_len = test_lenghts[:, 0]\n",
    "ts_trg_len = test_lenghts[:, 1]\n",
    "\n",
    "# Pair data and labels\n",
    "h_data = [tr_src_len, tr_trg_len, dev_src_len, dev_trg_len, ts_src_len, ts_trg_len]\n",
    "h_labels = ['tr_src_len', 'tr_trg_len', 'dev_src_len', 'dev_trg_len', 'ts_src_len', 'ts_trg_len']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_hist(ax, data, labels, title, bins=25):\n",
    "    # Draw the plot\n",
    "    ax.hist(data, bins=bins, density=True, label=labels, linewidth=0.5)\n",
    "\n",
    "    # Title and labels\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Lengths\")\n",
    "    ax.set_ylabel(\"Frenquency (normed)\")\n",
    "    ax.legend()\n",
    "\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10, 10]  # Fix size\n",
    "plt.rcParams['figure.dpi'] = 300  # Fix white bars (+linewidth=0.5)\n",
    "\n",
    "fig, axs = plt.subplots(4)\n",
    "get_hist(axs[0], h_data, h_labels, f\"Histogram of Train+Dev+Test\")\n",
    "get_hist(axs[1], h_data[0:2], h_labels[0:2], f\"Histogram of Train\")\n",
    "get_hist(axs[2], h_data[2:4], h_labels[2:4], f\"Histogram of Dev\")\n",
    "get_hist(axs[3], h_data[4:6], h_labels[4:6], f\"Histogram of Test\")\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10, 10]\n",
    "plt.subplots_adjust(hspace=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence length stats\n",
    "\n",
    "We can also get some statistical values from partitions to get a rough idea of how long is the average sequence or in which range of values we can find 95% of our data.\n",
    "\n",
    "By doing this, we are obtaining a value with which perform the sequence cut in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Get means\n",
    "train_mean_len = train_lenghts.mean(axis=0).round().astype(int)\n",
    "dev_mean_len = dev_lenghts.mean(axis=0).round().astype(int)\n",
    "test_mean_len = test_lenghts.mean(axis=0).round().astype(int)\n",
    "\n",
    "print(f\"Train => Mean length: src={train_mean_len[0]} | trg={train_mean_len[1]}\")\n",
    "print(f\"Dev => Mean length: src={dev_mean_len[0]} | trg={dev_mean_len[1]}\")\n",
    "print(f\"Test => Mean length: src={test_mean_len[0]} | trg={test_mean_len[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the length threshold\n",
    "\n",
    "Let's say that our model has only memory for sequences with a maximum of 150 tokens. With this value in mind (and the sequence length distribution), we can now exactly know how much data we are leaving behind. \n",
    "\n",
    "This is important so that we can get the most of our model by carefully tuning it with the statistical values that we have calculated. Additionally, since this is an educated process, we will also have a rough idea of how much our model could improve if we increase the computational resources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tr_src_p_score = stats.percentileofscore(train_lenghts[:, 0], MAX_SEQ_LENGTH)\n",
    "tr_trg_p_score = stats.percentileofscore(train_lenghts[:, 1], MAX_SEQ_LENGTH)\n",
    "print(f\"Train: percentile of {MAX_SEQ_LENGTH}: src={tr_src_p_score} | trg={tr_trg_p_score}\")\n",
    "\n",
    "dev_src_p_score = stats.percentileofscore(dev_lenghts[:, 0], MAX_SEQ_LENGTH)\n",
    "dev_trg_p_score = stats.percentileofscore(dev_lenghts[:, 1], MAX_SEQ_LENGTH)\n",
    "print(f\"Train: percentile of {MAX_SEQ_LENGTH}: src={dev_src_p_score} | trg={dev_trg_p_score}\")\n",
    "\n",
    "ts_src_p_score = stats.percentileofscore(test_lenghts[:, 0], MAX_SEQ_LENGTH)\n",
    "ts_trg_p_score = stats.percentileofscore(test_lenghts[:, 1], MAX_SEQ_LENGTH)\n",
    "print(f\"Train: percentile of {MAX_SEQ_LENGTH}: src={ts_src_p_score} | trg={ts_trg_p_score}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_percentiles(src_trg_lengths, lower_q, upper_q):\n",
    "    p = np.percentile(src_trg_lengths,  [lower_q, upper_q], axis=0)\n",
    "    (lb_tr_src_len, lb_tr_trg_len), (ub_tr_src_len, ub_tr_trg_len) = p\n",
    "\n",
    "    print(f\"- SRC ({lower_q}-{upper_q}%): {int(lb_tr_src_len)}-{int(ub_tr_src_len)}\")\n",
    "    print(f\"- TRG ({lower_q}-{upper_q}%): {int(lb_tr_trg_len)}-{int(ub_tr_trg_len)}\")\n",
    "\n",
    "    return p\n",
    "\n",
    "lower_q, upper_q = (0, 99.65)  # Percentage\n",
    "print(\"Percentile Train:\")\n",
    "tr_p = get_percentiles(train_lenghts, *(lower_q, upper_q))\n",
    "print(\"Percentile Dev:\")\n",
    "dev_p = get_percentiles(dev_lenghts, *(lower_q, upper_q))\n",
    "print(\"Percentile Test:\")\n",
    "ts_p = get_percentiles(test_lenghts, *(lower_q, upper_q))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Removing pairs\n",
    "\n",
    "NExt, we remove all pairs where either the *source* or *target* exceeds certain threshold.\n",
    "\n",
    "As we are deleting elements from a list while iterating, we need to start from the end.\n",
    "There are many ways to do this that are faster (e.g.: list comprehesion, creating a new copy,...), I have\n",
    "used this method because it simply works."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def remove_max_length(data, max_length):\n",
    "    removed = 0\n",
    "    for i in tqdm(reversed(range(len(data.examples))), total=len(data.examples)):\n",
    "        if len(data.examples[i].src) > max_length or len(data.examples[i].trg) > max_length:\n",
    "            del data.examples[i]\n",
    "            removed += 1\n",
    "    return removed"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f\"Train pairs (before): {len(train_data)}\")\n",
    "tr_removed = remove_max_length(train_data, MAX_SEQ_LENGTH)\n",
    "print(f\"Train pairs (now): {len(train_data)}\")\n",
    "print(f\"Train pairs (removed): {tr_removed}\")\n",
    "print(\"\")\n",
    "print(f\"Dev pairs (before): {len(dev_data)}\")\n",
    "print(f\"Dev pairs (now): {len(dev_data)}\")\n",
    "dev_removed = remove_max_length(dev_data, MAX_SEQ_LENGTH)\n",
    "print(f\"Dev pairs (removed): {dev_removed}\")\n",
    "print(\"\")\n",
    "print(f\"Test pairs (before): {len(test_data)}\")\n",
    "print(f\"Test pairs (now): {len(test_data)}\")\n",
    "ts_removed = remove_max_length(test_data, MAX_SEQ_LENGTH)\n",
    "print(f\"Test pairs (removed): {ts_removed}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Vocabulary\n",
    "\n",
    "For the next step we will build the vocabulary. That is, creating a list of unique words/tokens that our model will\n",
    "\"understand\". All the other words that are not in the vocabulary will be marked as `<unk>`.\n",
    "\n",
    "Technically, we cannot use the dev partition for anything but evaluating our model during training. We do this in\n",
    "order to avoid overfitting our model on the \"super secret\" test partition. Hance, we have to build the vocabulary\n",
    "using only the train partition.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define Train fields (they're already defined above)\n",
    "# SRC = data.Field(tokenize='spacy', tokenizer_language=SRC_LANG, init_token=SOS_WORD, eos_token=EOS_WORD, lower=True)\n",
    "# TRG = data.Field(tokenize='spacy', tokenizer_language=TRG_LANG, init_token=SOS_WORD, eos_token=EOS_WORD, lower=True)\n",
    "\n",
    "# Build vocabulary only with the training data\n",
    "SRC.build_vocab(train_data.src)\n",
    "TRG.build_vocab(train_data.trg)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Vocabulary size\n",
    "\n",
    "Once the vocabulary is built, we need to check its size and see if we need to perform some refinement.\n",
    "\n",
    "Anything between 10k to 15k seems reasonable. We could go for even 30k for big models."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f\"Words in SRC vocabulary: {len(SRC.vocab.freqs)}\")\n",
    "print(f\"Words in TRG vocabulary: {len(TRG.vocab.freqs)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Vocabulary refinement\n",
    "\n",
    "We can use all unique tokens found in the train partition as our \"vocabulary\", but this is usually not a good idea since\n",
    "many of the less frequent words in the vocabulary will be typos, personal names, words in another languages, etc.\n",
    "Moreover, using everything as our vocabulary would notable increase the resources needed.\n",
    "\n",
    "Luckily, there is a really simple solution to deal with this problem: Using the `k` most frequent words.\n",
    "\n",
    "From the words below, we see that everything is reasonable as since the vocabulary is not that big, we could leave\n",
    "it as is."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n=50\n",
    "print(\"Train vocab:\")\n",
    "print(\"=> SRC (most common): \" + str(SRC.vocab.freqs.most_common(n)))\n",
    "print(\"=> SRC (least common): \" + str(SRC.vocab.freqs.most_common()[-n-1:]))\n",
    "print(\"=> TRG (most common): \" + str(TRG.vocab.freqs.most_common(n)))\n",
    "print(\"=> TRG (least common): \" + str(TRG.vocab.freqs.most_common()[-n-1:]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Saving preprocessed dataset\n",
    "\n",
    "Since preprocessing and tokenizing a big dataset can take time, I recommend to save it always, if possible, in other\n",
    "to save us some time when debugging our model.\n",
    "\n",
    "For torchtext, we'll need to save the preprocessed examples for the train, dev and test partitions, along with their\n",
    "fields. Depending on the case we can ignore the last part. Here, I'll save only the train fields with their\n",
    "vocabularies.\n",
    "\n",
    "**Note:** *To decide the serialization approach I had to do some test as the naive pickle-version either doesn't work\n",
    "or is too slow.*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def save_examples(dataset, savepath):\n",
    "    start = time.time()\n",
    "\n",
    "    total = len(dataset.examples)\n",
    "    with open(savepath, 'w') as f:\n",
    "        # Save num. elements\n",
    "        f.write(json.dumps(total))\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "        # Save elements\n",
    "        for pair in tqdm(dataset.examples, total=total):\n",
    "            data = [pair.src, pair.trg]\n",
    "            f.write(json.dumps(data))\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "    end = time.time()\n",
    "    print(end - start)\n",
    "\n",
    "save_examples(train_data, f\"{DATASET_PATH}/tokenized/train.json\")\n",
    "save_examples(dev_data, f\"{DATASET_PATH}/tokenized/dev.json\")\n",
    "save_examples(test_data, f\"{DATASET_PATH}/tokenized/test.json\")\n",
    "print(\"Tokenized examples saved!\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading preprocessed dataset\n",
    "\n",
    "Similarly, we can also load the preprocessed datasets."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def load_examples(filename):\n",
    "    start = time.time()\n",
    "\n",
    "    examples = []\n",
    "    with open(filename, 'rb') as f:\n",
    "        # Read num. elements\n",
    "        line = f.readline()\n",
    "        total = json.loads(line)\n",
    "\n",
    "        # Save elements\n",
    "        for i in tqdm(range(total), total=total):\n",
    "            line = f.readline()\n",
    "            example = json.loads(line)\n",
    "            examples.append(example)\n",
    "\n",
    "    end = time.time()\n",
    "    print(end - start)\n",
    "    return examples\n",
    "\n",
    "\n",
    "train_data = load_examples(f\"{DATASET_PATH}/tokenized/train.json\")\n",
    "dev_data = load_examples(f\"{DATASET_PATH}/tokenized/dev.json\")\n",
    "test_data = load_examples(f\"{DATASET_PATH}/tokenized/test.json\")\n",
    "print(\"Tokenized examples loaded!\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Total pairs:\")\n",
    "print(f\"\\t- Train: {len(train_data.examples)}\")\n",
    "print(f\"\\t- Dev: {len(dev_data.examples)}\")\n",
    "print(f\"\\t- Test: {len(test_data.examples)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}